from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
import matplotlib.pyplot as plt
import numpy as np

# Load the Iris dataset
iris = load_iris()

# Select the first two classes (setosa and versicolor)
X = iris.data[0:100, :]


y = iris.target[0:100]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Simple logistic regression with two-class problem
lr1 = LogisticRegression()
lr1.fit(X_train, y_train)
ll1 = lr1.score(X_test, y_test)

# Multiple logistic regression with taking two-class problem
lr2 = LogisticRegression(multi_class='multinomial', solver='lbfgs')
lr2.fit(X_train, y_train)
ll2 = lr2.score(X_test, y_test)

# Multiple logistic regression with all classes problem
X = iris.data
y = iris.target
lr3 = LogisticRegression(multi_class='multinomial', solver='lbfgs')
lr3.fit(X_train, y_train)
ll3 = lr3.score(X_test, y_test)

# Stochastic gradient descent optimization
sgd = SGDClassifier(loss='log', max_iter=1000)
sgd.fit(X_train, y_train)
plt.plot(sgd.loss)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('SGD Loss vs. Epochs')
plt.show()
print('SGD Weights:', sgd.coef_)
print('SGD Bias:', sgd.intercept_)

# Batched gradient descent optimization
# ...


# Mini-batched gradient descent optimization
# ...

# Newton-Raphson optimization
# ...


# Plot accuracy vs. epochs graphs
# ...

# Print optimized weights and bias
# ...

# Repeat for each optimization algorithm
